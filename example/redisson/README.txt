加锁流程
1. 选择一个redis master
  a. 首先会做一个判断,判断我们的key是否包含{}, 如果包含{}, 那么就用只用这块做crc16的计算, 否则就用整个key做crc16计算
  b. crc16计算后是一个int类型的值, 用这个值与16384取模, 判断这个key应该放到哪个槽里
  c. 现在已经知道这个key要放到哪个槽里了, 然后redisson里面保存了每一个槽对应的redis节点信息, 放在了一个AtomicReferenceArray中, 从里面取出对应槽对应的节点信息
2. 准备执行lua脚本, 设置分布式锁
  a. 这个锁是一个hash结构, 就是类似于我们Java中的map
  b. key是我们自己设置的那个锁的名字
  c. hash中的field就是 当前redisson客户端的连接管理器的id(就是个UUID) + 当前线程id组成
  d. value中就是这个锁重入的次数
3. 此时有3种情况
  a. 这个key不存在
    ⅰ. 直接把自己设置进入, 并且设置key的超时时间, 默认是30s
  b. 这个key存在, 但是自己的连接id+线程id 在这个hash的属性中, 也就是持有锁的是自己
    ⅰ. 把属性的值+1, 也就是重入一次, 然后把超时时间重新设置一下
  c. 上面两种都不是, 也就是持有锁的人不是自己
    ⅰ. 返回这个key超时的毫秒数
4. 如果刚刚执行的lua脚本返回的是null, 也就是说现在持有锁的是自己, 那么就启动一个看门狗(后台线程),
默认每个10秒钟去续命一次, 当然了, 可以续命的前提是这把锁还在, 如果因为gc或者网络等问题, 导致锁超时,
那么看门狗就无法续命了(需要注意的是, 看门狗只会在我们没有指定超时时间的时候帮我们续命, 如果自己指定了超时时间, 那么看门狗是没法帮我们续命的)
5. 如果返回的不是null, 也就是自己没有获取到锁, 那么就搞了一个发布订阅模式, 作为监听, 如果其他线程释放了锁,
自己再去竞争锁, 当然了, 它也不会一直傻傻的在订阅, 如果持有锁的客户端挂了, 那没人给自己发布一个消息,
所以自己也搞了一个超时时间, 就是用刚刚lua脚本返回的时间作为超时时间, 超时后再来竞争一下锁试试.


解锁流程
1. 会判断当前这个锁在不在, 如果不在, 那么直接发送一个订阅消息 0, 说这个锁已经被解开了
2. 如果这把锁存在, 但是不是自己持有, 那么就什么也不做
3. 如果上面两种情况都不符合, 那么就会把这个key中自己对应的属性的值-1
4. 如果-1完成后, 大于0, 那么说明这个锁还没减完
5. 如果-1后不大于0, 那么就说明这把锁已经被释放了, 那么就删除这个锁, 然后pushlish一下, 通知其他人, 这把锁已经被释放了

#####################################

Redisson锁自动"续命"源码
private void scheduleExpirationRenewal(final long threadId);
是在加锁后开启一个守护线程进行监听. Redisson超时时间默认设置30s,
线程每10s调用一次判断锁还是否存在, 如果存在则延长锁的超时时间.
现在，我们再回过头来看看redisson中的加锁代码与原理图, 其实完善到这种程度已经可以满足很多公司的使用了,
并且很多公司也确实是这样用的.

但我们再思考下是否还存在问题呢? 例如以下场景:

众所周知 Redis 在实际部署使用时都是集群部署的, 那在高并发场景下我们加锁, 当把key写入到master节点后,
master还未同步到slave节点时master宕机了, 原有的slave节点经过选举变为了新的master节点, 此时可能就会出现锁失效问题.

通过分布式锁的实现机制我们知道, 高并发场景下只有加锁成功的请求可以继续处理业务逻辑.
那就出现了大伙都来加锁, 但有且仅有一个加锁成功了, 剩余的都在等待.
其实分布式锁与高并发在语义上就是相违背的, 我们的请求虽然都是并发, 但Redis帮我们把请求进行了排队执行,
也就是把我们的并行转为了串行. 串行执行的代码肯定不存在并发问题了, 但是程序的性能肯定也会因此受到影响.

针对这些问题, 我们再次思考解决方案:

在思考解决方案时我们首先想到CAP原则（一致性、可用性、分区容错性）,
那么现在的Redis就是满足AP(可用性、分区容错性), 如果想要解决该问题我们就需要寻找满足CP(一致性、分区容错性)的分布式系统.
首先想到的就是zookeeper, zookeeper的集群间数据同步机制是当主节点接收数据后不会立即返回给客户端成功的反馈,
它会先与子节点进行数据同步, 半数以上的节点都完成同步后才会通知客户端接收成功.
并且如果主节点宕机后, 根据zookeeper的Zab协议（Zookeeper原子广播）重新选举的主节点一定是已经同步成功的.

那么问题来了, Redisson与zookeeper分布式锁我们如何选择呢?
答案是如果并发量没有那么高, 可以用zookeeper来做分布式锁, 但是它的并发能力远远不如Redis.
如果你对并发要求比较高的话, 那就用Redis, 偶尔出现的主从架构锁失效的问题其实是可以容忍的.

关于第二个提升性能的问题, 我们可以参考ConcurrentHashMap的锁分段技术的思想,
例如我们代码的库存量当前为1000, 那我们可以分为10段, 每段100, 然后对每段分别加锁,
这样就可以同时执行10个请求的加锁与处理, 当然有要求的同学还可以继续细分.
但其实Redis的qps已经达到10W+了, 没有特别高并发量的场景下也是完全够用的.

#####################################

